// src/lib/functions.ts
import { inngest } from "./client";
import { createClient } from "@supabase/supabase-js";
import type { Database } from "@/lib/supabase";
// E2B sandbox SDK (JS/TS v2)
import { Sandbox, SandboxInfo } from "@e2b/code-interpreter";

// ---------------------------------------------------------------------------
// Supabase setup
// ---------------------------------------------------------------------------
const SUPABASE_URL = process.env.NEXT_PUBLIC_SUPABASE_URL ?? "";
const SUPABASE_ANON_KEY = process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY ?? "";
export const supabase = createClient<Database>(SUPABASE_URL, SUPABASE_ANON_KEY);

// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------
export type MessageRole = "user" | "assistant" | "system";

export interface MessageInsert {
  chat_id: string;
  role: MessageRole;
  content: string;
  created_at?: string;
}

export interface ModelConfig {
  type: "nlp" | "cv" | "tabular";
  task: string;
  baseModel: string;
  dataset: string;
  epochs?: number;
  batchSize?: number;
  learningRate?: number;
}

// ---------------------------------------------------------------------------
// Analyze Prompt â†’ Select Model & Dataset
// ---------------------------------------------------------------------------
export function analyzePrompt(prompt: string): ModelConfig {
  const text = (prompt || "").toLowerCase();

  // Vision
  if (/(image|photo|vision|classify images|image classification)/.test(text))
    return {
      type: "cv",
      task: "image-classification",
      baseModel: "google/vit-base-patch16-224",
      dataset: "beans",
    };

  if (/(object detection|detect objects|bounding box|yolo|detr)/.test(text))
    return {
      type: "cv",
      task: "object-detection",
      baseModel: "facebook/detr-resnet-50",
      dataset: "coco",
    };

  // NLP
  if (/(sentiment|emotion)/.test(text))
    return {
      type: "nlp",
      task: "sentiment-analysis",
      baseModel: "distilbert-base-uncased",
      dataset: "imdb",
    };

  if (/(news|topic|classify|text classification)/.test(text))
    return {
      type: "nlp",
      task: "text-classification",
      baseModel: "distilbert-base-uncased",
      dataset: "ag_news",
    };

  if (/(question answering|qa)/.test(text))
    return {
      type: "nlp",
      task: "question-answering",
      baseModel: "distilbert-base-cased-distilled-squad",
      dataset: "squad",
    };

  if (/(summarization|summarise|summarize)/.test(text))
    return {
      type: "nlp",
      task: "summarization",
      baseModel: "facebook/bart-large-cnn",
      dataset: "cnn_dailymail",
    };

  if (/(ner|named entity|entity recognition)/.test(text))
    return {
      type: "nlp",
      task: "token-classification",
      baseModel: "dslim/bert-base-NER",
      dataset: "conll2003",
    };

  if (/(translate|translation)/.test(text))
    return {
      type: "nlp",
      task: "translation",
      baseModel: "Helsinki-NLP/opus-mt-en-de",
      dataset: "wmt14",
    };

  // Tabular
  if (/(csv|tabular|columns|features)/.test(text))
    return {
      type: "tabular",
      task: "tabular-classification",
      baseModel: "xgboost",
      dataset: "adult",
    };

  // Default
  return {
    type: "nlp",
    task: "text-classification",
    baseModel: "distilbert-base-uncased",
    dataset: "ag_news",
  };
}

// ---------------------------------------------------------------------------
// Dataset Selector (Hugging Face â†’ Kaggle fallback)
// ---------------------------------------------------------------------------
export async function findDataset(
  cfg: ModelConfig
): Promise<{ name: string; source: string; url: string | null }> {
  const hfToken = process.env.HF_ACCESS_TOKEN || process.env.HUGGINGFACE_TOKEN;
  const query = cfg.task.replace(/-/g, " ");

  let chosen = { name: cfg.dataset, source: "default", url: null as string | null };

  try {
    const res = await fetch(
      `https://huggingface.co/api/datasets?search=${encodeURIComponent(query)}`,
      { headers: hfToken ? { Authorization: `Bearer ${hfToken}` } : {} }
    );

    if (res.ok) {
      const arr = await res.json();
      const match = arr.find((d: any) =>
        (d.id || "").toLowerCase().includes(cfg.dataset.toLowerCase())
      );
      if (match)
        chosen = {
          name: match.id,
          source: "huggingface",
          url: `https://huggingface.co/datasets/${match.id}`,
        };
    }
  } catch {
    // Fallback to Kaggle
  }

  if (!chosen.url) {
    const kaggleMap: Record<string, { name: string; url: string }> = {
      "sentiment-analysis": {
        name: "imdb",
        url: "https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews",
      },
      "text-classification": {
        name: "ag_news",
        url: "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset",
      },
      "image-classification": {
        name: "cifar-10",
        url: "https://www.kaggle.com/datasets/cifar-10",
      },
      "object-detection": {
        name: "coco",
        url: "https://www.kaggle.com/datasets/awsaf49/coco-2017-dataset",
      },
      "question-answering": {
        name: "squad",
        url: "https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset",
      },
      summarization: {
        name: "cnn_dailymail",
        url: "https://www.kaggle.com/datasets/snapcrack/all-the-news",
      },
      "token-classification": {
        name: "conll2003",
        url: "https://www.kaggle.com/datasets/abhinavwalia95/entity-annotated-corpus",
      },
      translation: {
        name: "wmt14",
        url: "https://www.kaggle.com/datasets/yashwantsingh/wmt14-english-german",
      },
      "tabular-classification": {
        name: "adult",
        url: "https://www.kaggle.com/datasets/uciml/adult-census-income",
      },
    };
    if (kaggleMap[cfg.task]) {
      chosen = {
        name: kaggleMap[cfg.task].name,
        source: "kaggle",
        url: kaggleMap[cfg.task].url,
      };
    }
  }

  return chosen;
}

// ---------------------------------------------------------------------------
// Generators
// ---------------------------------------------------------------------------
export const genRequirements = (): string =>
  [
    "torch>=2.1.0",
    "transformers>=4.40.0",
    "datasets>=2.19.0",
    "accelerate>=0.30.0",
    "scikit-learn>=1.4.0",
    "pandas>=2.2.0",
    "numpy>=1.26.0",
    "fastapi>=0.110.0",
    "uvicorn>=0.29.0",
    "gradio>=4.31.0",
    "huggingface_hub>=0.23.0",
    "requests>=2.32.0",
  ].join("\n");

// Generate training code based on model config
export const generateTrainingCode = (cfg: ModelConfig): string => {
  if (cfg.task === "sentiment-analysis" || cfg.task === "text-classification") {
    return `
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset
import json

# Load config
with open('model_config.json', 'r') as f:
    config = json.load(f)

print(f"ðŸš€ Starting training for {config['task']}...")
print(f"ðŸ“Š Dataset: {config['dataset']}")
print(f"ðŸ¤– Base Model: {config['baseModel']}")

# Load dataset
dataset = load_dataset("${cfg.dataset}", split="train[:1000]")  # Small subset for demo
dataset = dataset.train_test_split(test_size=0.2)

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("${cfg.baseModel}")
model = AutoModelForSequenceClassification.from_pretrained("${cfg.baseModel}", num_labels=2)

# Tokenize dataset
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=${cfg.epochs || 3},
    per_device_train_batch_size=${cfg.batchSize || 16},
    per_device_eval_batch_size=16,
    warmup_steps=100,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
)

# Train
print("ðŸ‹ï¸ Training started...")
trainer.train()
print("âœ… Training complete!")

# Save model
model.save_pretrained("./model")
tokenizer.save_pretrained("./model")
print("ðŸ’¾ Model saved!")
`;
  }
  
  // Default simple training script
  return `
import time
print("ðŸš€ Training started...")
time.sleep(5)
print("âœ… Training complete!")
`;
};

// Generate FastAPI deployment code
export const generateDeploymentCode = (cfg: ModelConfig): string => {
  if (cfg.task === "sentiment-analysis" || cfg.task === "text-classification") {
    return `
from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

app = FastAPI(title="${cfg.task} API", version="1.0.0")

# Load model
try:
    tokenizer = AutoTokenizer.from_pretrained("./model")
    model = AutoModelForSequenceClassification.from_pretrained("./model")
    model.eval()
    print("âœ… Model loaded successfully!")
except:
    # Fallback to base model if training didn't complete
    tokenizer = AutoTokenizer.from_pretrained("${cfg.baseModel}")
    model = AutoModelForSequenceClassification.from_pretrained("${cfg.baseModel}", num_labels=2)
    model.eval()
    print("âš ï¸ Using base model (training may not have completed)")

class PredictionRequest(BaseModel):
    text: str

class PredictionResponse(BaseModel):
    text: str
    label: str
    confidence: float

@app.get("/")
def read_root():
    return {
        "message": "ðŸ¤– ${cfg.task} API is running!",
        "model": "${cfg.baseModel}",
        "task": "${cfg.task}",
        "endpoints": {
            "predict": "/predict",
            "health": "/health"
        }
    }

@app.get("/health")
def health_check():
    return {"status": "healthy", "model_loaded": True}

@app.post("/predict", response_model=PredictionResponse)
def predict(request: PredictionRequest):
    # Tokenize input
    inputs = tokenizer(request.text, return_tensors="pt", truncation=True, max_length=128)
    
    # Get prediction
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
        confidence, predicted_class = torch.max(predictions, dim=1)
    
    # Map to label
    labels = ["negative", "positive"]
    label = labels[predicted_class.item()]
    
    return PredictionResponse(
        text=request.text,
        label=label,
        confidence=float(confidence.item())
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
`;
  }
  
  // Default API
  return `
from fastapi import FastAPI

app = FastAPI()

@app.get("/")
def read_root():
    return {"message": "Model API is running!", "task": "${cfg.task}"}

@app.post("/predict")
def predict(data: dict):
    return {"result": "prediction", "input": data}
`;
};

// ---------------------------------------------------------------------------
// Inngest Functions
// ---------------------------------------------------------------------------
export const testFunction = inngest.createFunction(
  { id: "test-function", name: "Test Function" },
  { event: "test/ping" },
  async () => ({ success: true, message: "Inngest working âœ…" })
);

export const generateModelCode = inngest.createFunction(
  { id: "ai-workspace-generate-model", name: "Generate + Train + Deploy", concurrency: { limit: 2 } },
  { event: "ai/model.generate" },
  async ({ event, step }) => {
    const { prompt = "", chatId, userId } = event.data as {
      prompt?: string;
      chatId?: string;
      userId?: string;
    };

    // Step 1: Analyze prompt
    const analysis = await step.run("analyze-prompt", async () => {
      if (chatId) {
        await (supabase.from as any)("messages").insert({
          chat_id: chatId,
          role: "assistant",
          content: "ðŸ” **Step 1/7**: Analyzing your prompt...\n\nI'm understanding what kind of model you need and selecting the best architecture.",
        } as any);
      }
      return analyzePrompt(prompt);
    });

    // Step 2: Select dataset
    const dataset = await step.run("select-dataset", async () => {
      if (chatId) {
        await (supabase.from as any)("messages").insert({
          chat_id: chatId,
          role: "assistant",
          content: `ðŸ“Š **Step 2/7**: Finding the perfect dataset...\n\nSearching for **${analysis.task}** datasets on Hugging Face and Kaggle.`,
        } as any);
      }
      return findDataset(analysis);
    });
    const cfg: ModelConfig = { ...analysis, dataset: dataset.name, epochs: 3, batchSize: 16 };

    // Step 3: Create E2B sandbox
    const secure = process.env.E2B_SECURE === "false" ? false : true;
    const sandboxHandle = await step.run("e2b-create-sandbox", async () => {
      if (chatId) {
        await (supabase.from as any)("messages").insert({
          chat_id: chatId,
          role: "assistant",
          content: `âš¡ **Step 3/7**: Creating E2B sandbox environment...\n\nSetting up isolated Python environment with GPU acceleration for training.`,
        } as any);
      }
      const sb = await Sandbox.create({ secure, timeoutMs: 600000 }); // 10 min timeout
      return sb;
    });

    const sandboxId = (sandboxHandle as any).sandboxId || (sandboxHandle as any).id;

    // Step 4: Generate complete training code
    const trainingCode = generateTrainingCode(cfg);
    const deploymentCode = generateDeploymentCode(cfg);

    if (chatId) {
      await step.run("send-code-generation-message", async () => {
        await (supabase.from as any)("messages").insert({
          chat_id: chatId,
          role: "assistant",
          content: `ðŸ’» **Step 4/7**: Generating ML code...\n\nâœ… Created training script for **${cfg.task}**\nâœ… Generated FastAPI deployment code\nâœ… Configured **${cfg.baseModel}** model\nâœ… Set up **${cfg.dataset}** dataset loader`,
        } as any);
      });
    }

    // Step 5: Write all files to sandbox
    await step.run("e2b-write-files", async () => {
      if (chatId) {
        await (supabase.from as any)("messages").insert({
          chat_id: chatId,
          role: "assistant",
          content: `ðŸ“ **Step 5/7**: Writing files to sandbox...\n\nâœ… requirements.txt\nâœ… train.py\nâœ… app.py (FastAPI server)\nâœ… model_config.json`,
        } as any);
      }
      // E2B v2 API: write files individually
      await (sandboxHandle as any).files.write("/workspace/requirements.txt", genRequirements());
      await (sandboxHandle as any).files.write("/workspace/train.py", trainingCode);
      await (sandboxHandle as any).files.write("/workspace/app.py", deploymentCode);
      await (sandboxHandle as any).files.write("/workspace/model_config.json", JSON.stringify(cfg, null, 2));
      return true;
    });

    // Step 6: Install dependencies
    await step.run("e2b-install-deps", async () => {
      if (chatId) {
        await (supabase.from as any)("messages").insert({
          chat_id: chatId,
          role: "assistant",
          content: `ðŸ“¦ **Step 6/7**: Installing dependencies...\n\nInstalling PyTorch, Transformers, FastAPI, and other required packages. This may take 2-3 minutes...`,
        } as any);
      }
      const result = await (sandboxHandle as any).commands.run(
        "cd /workspace && pip install -r requirements.txt",
        { timeout: 300000 } // 5 min timeout
      );
      console.log(`[${sandboxId}] Dependencies installed:`, result.stdout);
      return { success: true, output: result.stdout };
    });

    // Step 7: Run training
    const trainingResult = await step.run("e2b-train-model", async () => {
      if (chatId) {
        await (supabase.from as any)("messages").insert({
          chat_id: chatId,
          role: "assistant",
          content: `ðŸ‹ï¸ **Step 7/7**: Training your model...\n\nâš¡ Loading **${cfg.baseModel}** base model\nðŸ“Š Training on **${cfg.dataset}** dataset\nðŸ”¥ Running for **${cfg.epochs}** epochs\n\nThis will take 3-5 minutes. Training in progress...`,
        } as any);
      }
      const result = await (sandboxHandle as any).commands.run(
        "cd /workspace && python train.py",
        { 
          timeout: 600000, // 10 min timeout
          onStdout: (data: string) => console.log(`[${sandboxId}] Training:`, data),
          onStderr: (data: string) => console.error(`[${sandboxId}] Error:`, data),
        }
      );
      return { 
        success: result.exitCode === 0, 
        output: result.stdout,
        error: result.stderr 
      };
    });

    // Step 8: Start FastAPI server in background
    const deploymentUrl = await step.run("e2b-deploy-api", async () => {
      // Start FastAPI server in background
      const command = await (sandboxHandle as any).commands.run(
        "cd /workspace && python -m uvicorn app:app --host 0.0.0.0 --port 8000",
        {
          background: true,
          onStdout: (data: string) => console.log(`[${sandboxId}] Server:`, data),
        }
      );

      // Wait for server to start
      await step.sleep("wait-for-server", "5s");

      // Get the public URL
      const url = `https://${sandboxId}.e2b.dev`;
      return url;
    });

    // Step 9: Save to database
    if (userId) {
      await step.run("save-to-db", async () => {
        await (supabase.from('ai_models').insert as any)({
          user_id: userId,
          name: `${cfg.task} Model`,
          description: `${cfg.task} model trained on ${cfg.dataset}`,
          model_type: cfg.task,
          framework: 'pytorch',
          base_model: cfg.baseModel,
          dataset_source: dataset.source,
          dataset_name: cfg.dataset,
          training_status: 'deployed',
          deployed_at: new Date().toISOString(),
          metadata: {
            sandboxId,
            deploymentUrl,
            trainingResult,
          }
        });
      });
    }

    // Step 10: Send completion message
    if (chatId) {
      await step.run("send-completion-message", async () => {
        const msg: MessageInsert = {
          chat_id: chatId,
          role: "assistant",
          content: `ðŸŽ‰ **DEPLOYMENT COMPLETE!**

---

## âœ… Your Model is Live!

**ðŸš€ Live API URL**: [${deploymentUrl}](${deploymentUrl})
**ðŸ“¦ Sandbox ID**: \`${sandboxId}\`
**ðŸ¤– Model**: ${cfg.baseModel}
**ðŸ“Š Dataset**: ${cfg.dataset}
**âš¡ Task**: ${cfg.task}

---

## ðŸ§ª Test Your Model

**Option 1: Browser**
Visit: [${deploymentUrl}](${deploymentUrl})

**Option 2: cURL**
\`\`\`bash
curl -X POST "${deploymentUrl}/predict" \\
  -H "Content-Type: application/json" \\
  -d '{"text": "This product is amazing! I love it!"}'
\`\`\`

**Option 3: Python**
\`\`\`python
import requests

response = requests.post(
    "${deploymentUrl}/predict",
    json={"text": "This is great!"}
)
print(response.json())
\`\`\`

---

## ðŸ“¥ Download Source Code

All training code, model files, and deployment scripts are available in the E2B sandbox!

**Your model is now live and ready for production use!** ðŸŽ¯`,
        };
        await (supabase.from as any)("messages").insert(msg as any);
      });
    }

    return { 
      success: true, 
      config: cfg, 
      sandboxId, 
      deploymentUrl,
      trainingResult 
    };
  }
);

// Stub training
export const trainAIModel = inngest.createFunction(
  { id: "ai-train-model", name: "Train AI Model", concurrency: { limit: 1 } },
  { event: "ai/model.train" },
  async ({ event }) => {
    const { modelConfig } = event.data as { modelConfig: ModelConfig };
    return { success: true, received: modelConfig };
  }
);

// Stub deploy
export const deployToE2B = inngest.createFunction(
  { id: "ai-deploy-e2b", name: "Deploy to E2B", concurrency: { limit: 3 } },
  { event: "ai/model.deploy-e2b" },
  async ({ event, step }) => {
    const secure = process.env.E2B_SECURE === "false" ? false : true;

    // Paginated listing example (v2)
    const list = await step.run("e2b-list-sandboxes", async () => {
      const paginator = Sandbox.list({ query: {} });
      const collected: SandboxInfo[] = [];
      while (paginator.hasNext) {
        const items = await paginator.nextItems();
        collected.push(...items);
        if (collected.length >= 50) break; // limit reasonable amount
      }
      return collected.map((s) => ({ id: (s as any).sandboxId || (s as any).id, state: (s as any).state }));
    });

    return { success: true, sandboxes: list };
  }
);
