import { NextRequest, NextResponse } from 'next/server'
import JSZip from 'jszip'

/**
 * AI Model Download API
 * Provides downloadable ZIP of generated ML files
 */

export async function GET(
  request: NextRequest,
  { params }: { params: Promise<{ eventId: string }> }
) {
  try {
    const { eventId } = await params;
    
    if (!eventId) {
      return NextResponse.json({ error: 'Event ID required' }, { status: 400 });
    }

    console.log(`üì¶ Creating download package for eventId: ${eventId}`);

    // Generate all the ML files (same as in Inngest function)
    const modelConfig = {
      task: 'Sentiment Analysis',
      type: 'text-classification',
      baseModel: 'cardiffnlp/twitter-roberta-base-sentiment-latest'
    };

    const datasetInfo = {
      name: 'IMDB Movie Reviews',
      source: 'huggingface',
      hfDataset: 'imdb'
    };

    const files = {
      'main.py': generateFastAPIApp(modelConfig, datasetInfo),
      'train.py': generateE2BTrainingScript(modelConfig, datasetInfo),
      'model.py': generateModelArchitecture(modelConfig),
      'dataset.py': generateDatasetLoader(modelConfig, datasetInfo),
      'inference.py': generateInferenceScript(modelConfig),
      'config.py': generateConfigScript(modelConfig),
      'utils.py': generateUtilsScript(modelConfig),
      'requirements.txt': generateRequirements(modelConfig),
      'README.md': generateREADME(modelConfig, 'sentiment analysis model'),
      'Dockerfile': generateDockerfile(modelConfig),
      'index.html': generateHTMLInterface(modelConfig),
      'setup.sh': generateSetupScript(modelConfig, datasetInfo)
    };

    // Create ZIP file
    const zip = new JSZip();
    
    // Add all files to ZIP
    Object.entries(files).forEach(([filename, content]) => {
      zip.file(filename, content);
    });

    // Add project info
    const projectInfo = {
      eventId,
      generatedAt: new Date().toISOString(),
      modelType: modelConfig.task,
      framework: 'PyTorch + Transformers',
      files: Object.keys(files),
      instructions: {
        setup: 'Run: pip install -r requirements.txt',
        train: 'Run: python train.py',
        serve: 'Run: python main.py',
        docker: 'Run: docker build -t ai-model . && docker run -p 8000:8000 ai-model'
      }
    };
    
    zip.file('project-info.json', JSON.stringify(projectInfo, null, 2));

    // Generate ZIP as base64 and convert to buffer
    const zipBase64 = await zip.generateAsync({ type: 'base64' });
    const zipBuffer = Buffer.from(zipBase64, 'base64');

    // Return ZIP file
    return new NextResponse(zipBuffer, {
      headers: {
        'Content-Type': 'application/zip',
        'Content-Disposition': `attachment; filename="ai-model-${eventId.slice(-8)}.zip"`,
        'Content-Length': zipBuffer.length.toString()
      }
    });

  } catch (error: any) {
    console.error('Download API error:', error);
    return NextResponse.json(
      { error: error.message || 'Failed to create download' },
      { status: 500 }
    );
  }
}

// File generation functions (simplified versions)
function generateFastAPIApp(modelConfig: any, datasetInfo: any): string {
  return `"""
FastAPI Application for ${modelConfig.task}
Generated by zehanx AI
"""

from fastapi import FastAPI, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
import uvicorn
from transformers import pipeline
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="${modelConfig.task} API")
app.mount("/static", StaticFiles(directory="."), name="static")

# Load model
try:
    model_pipeline = pipeline("text-classification", model="${modelConfig.baseModel}")
    logger.info("‚úÖ Model loaded successfully")
except Exception as e:
    logger.error(f"‚ùå Model loading failed: {e}")
    model_pipeline = None

class PredictionRequest(BaseModel):
    text: str

@app.get("/", response_class=HTMLResponse)
async def get_interface():
    with open("index.html", "r") as f:
        return HTMLResponse(content=f.read())

@app.post("/predict")
async def predict(request: PredictionRequest):
    if model_pipeline is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    results = model_pipeline(request.text)
    result = results[0] if isinstance(results, list) else results
    
    return {
        "prediction": result["label"],
        "confidence": result["score"],
        "text": request.text
    }

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000)
`;
}

function generateE2BTrainingScript(modelConfig: any, datasetInfo: any): string {
  return `"""
Training Script for ${modelConfig.task}
Generated by zehanx AI
"""

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset
import numpy as np
from sklearn.metrics import accuracy_score

def train_model():
    print("üöÄ Starting training...")
    
    # Load model and tokenizer
    tokenizer = AutoTokenizer.from_pretrained("${modelConfig.baseModel}")
    model = AutoModelForSequenceClassification.from_pretrained("${modelConfig.baseModel}", num_labels=2)
    
    # Load dataset
    dataset = load_dataset("${datasetInfo.hfDataset}")
    
    def tokenize_function(examples):
        return tokenizer(examples['text'], truncation=True, padding=True, max_length=512)
    
    # Prepare data
    train_dataset = dataset['train'].map(tokenize_function, batched=True).select(range(1000))
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir='./results',
        num_train_epochs=3,
        per_device_train_batch_size=8,
        warmup_steps=100,
        weight_decay=0.01,
        logging_dir='./logs',
    )
    
    def compute_metrics(eval_pred):
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        return {'accuracy': accuracy_score(labels, predictions)}
    
    # Create trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        compute_metrics=compute_metrics,
        tokenizer=tokenizer
    )
    
    # Train
    trainer.train()
    
    # Save model
    trainer.save_model('./trained_model')
    tokenizer.save_pretrained('./trained_model')
    
    print("‚úÖ Training completed!")

if __name__ == "__main__":
    train_model()
`;
}

function generateModelArchitecture(modelConfig: any): string {
  return `"""
Model Architecture for ${modelConfig.task}
"""

import torch
import torch.nn as nn
from transformers import AutoModel, AutoModelForSequenceClassification

class CustomModel(nn.Module):
    def __init__(self, model_name="${modelConfig.baseModel}", num_labels=2):
        super().__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.3)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
        
    def forward(self, input_ids, attention_mask=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        output = self.dropout(pooled_output)
        return self.classifier(output)

def create_model():
    return AutoModelForSequenceClassification.from_pretrained("${modelConfig.baseModel}", num_labels=2)
`;
}

function generateDatasetLoader(modelConfig: any, datasetInfo: any): string {
  return `"""
Dataset Loader for ${modelConfig.task}
"""

from datasets import load_dataset
import pandas as pd

def load_data():
    try:
        dataset = load_dataset("${datasetInfo.hfDataset}")
        return dataset
    except Exception as e:
        print(f"Error loading dataset: {e}")
        return None

if __name__ == "__main__":
    data = load_data()
    print(f"Dataset loaded: {data}")
`;
}

function generateInferenceScript(modelConfig: any): string {
  return `"""
Inference Script for ${modelConfig.task}
"""

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

class ModelInference:
    def __init__(self, model_path='./trained_model'):
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
            print("‚úÖ Custom model loaded")
        except:
            self.pipeline = pipeline("text-classification", model="${modelConfig.baseModel}")
            print("‚úÖ Pre-trained model loaded")
    
    def predict(self, text):
        if hasattr(self, 'pipeline'):
            return self.pipeline(text)
        else:
            inputs = self.tokenizer(text, return_tensors='pt', truncation=True, padding=True)
            with torch.no_grad():
                outputs = self.model(**inputs)
                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
                return predictions.numpy()

if __name__ == "__main__":
    inference = ModelInference()
    result = inference.predict("This is a test")
    print(result)
`;
}

function generateConfigScript(modelConfig: any): string {
  return `"""
Configuration for ${modelConfig.task}
"""

from dataclasses import dataclass

@dataclass
class ModelConfig:
    model_name: str = "${modelConfig.baseModel}"
    task: str = "${modelConfig.task}"
    num_labels: int = 2
    max_length: int = 512
    epochs: int = 3
    batch_size: int = 8
    learning_rate: float = 2e-5
    
config = ModelConfig()
`;
}

function generateUtilsScript(modelConfig: any): string {
  return `"""
Utility Functions for ${modelConfig.task}
"""

import torch
import logging
import os
from datetime import datetime

def setup_logging():
    logging.basicConfig(level=logging.INFO)
    return logging.getLogger(__name__)

def save_model(model, tokenizer, path):
    os.makedirs(path, exist_ok=True)
    model.save_pretrained(path)
    tokenizer.save_pretrained(path)
    print(f"‚úÖ Model saved to {path}")

def get_device():
    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')

logger = setup_logging()
`;
}

function generateRequirements(modelConfig: any): string {
  return `torch>=1.9.0
transformers>=4.21.0
fastapi>=0.70.0
uvicorn>=0.15.0
datasets>=2.0.0
pandas>=1.3.0
numpy>=1.21.0
scikit-learn>=1.0.0
pydantic>=1.8.0
python-multipart>=0.0.5`;
}

function generateREADME(modelConfig: any, prompt: string): string {
  return `# ${modelConfig.task} Model

Generated by zehanx AI

## Description
${prompt}

## Quick Start

1. Install dependencies:
\`\`\`bash
pip install -r requirements.txt
\`\`\`

2. Run the FastAPI server:
\`\`\`bash
python main.py
\`\`\`

3. Open http://localhost:8000 in your browser

## Training

To train the model:
\`\`\`bash
python train.py
\`\`\`

## Files

- \`main.py\` - FastAPI web server
- \`train.py\` - Training script
- \`model.py\` - Model architecture
- \`dataset.py\` - Data loading
- \`inference.py\` - Inference utilities
- \`config.py\` - Configuration
- \`utils.py\` - Helper functions
- \`index.html\` - Web interface
- \`Dockerfile\` - Docker configuration

Built with ‚ù§Ô∏è by zehanx AI
`;
}

function generateDockerfile(modelConfig: any): string {
  return `FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["python", "main.py"]`;
}

function generateHTMLInterface(modelConfig: any): string {
  return `<!DOCTYPE html>
<html>
<head>
    <title>${modelConfig.task}</title>
    <style>
        body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }
        .header { text-align: center; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px; margin-bottom: 30px; }
        textarea { width: 100%; padding: 15px; border: 2px solid #ddd; border-radius: 8px; font-size: 16px; }
        button { background: #667eea; color: white; border: none; padding: 15px 30px; font-size: 16px; border-radius: 8px; cursor: pointer; }
        button:hover { background: #5a67d8; }
        .result { margin-top: 20px; padding: 20px; background: #f7fafc; border-radius: 8px; }
    </style>
</head>
<body>
    <div class="header">
        <h1>${modelConfig.task}</h1>
        <p>Powered by zehanx AI</p>
    </div>
    
    <div>
        <label>Enter text to analyze:</label>
        <textarea id="textInput" rows="4" placeholder="Type your text here..."></textarea>
        <br><br>
        <button onclick="predict()">Analyze</button>
    </div>
    
    <div id="result" class="result" style="display: none;"></div>

    <script>
        async function predict() {
            const text = document.getElementById('textInput').value;
            if (!text.trim()) return;
            
            try {
                const response = await fetch('/predict', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ text: text })
                });
                
                const result = await response.json();
                
                document.getElementById('result').innerHTML = \`
                    <h3>Results:</h3>
                    <p><strong>Prediction:</strong> \${result.prediction}</p>
                    <p><strong>Confidence:</strong> \${(result.confidence * 100).toFixed(1)}%</p>
                \`;
                document.getElementById('result').style.display = 'block';
                
            } catch (error) {
                alert('Error: ' + error.message);
            }
        }
    </script>
</body>
</html>`;
}

function generateSetupScript(modelConfig: any, datasetInfo: any): string {
  return `#!/bin/bash
echo "Setting up ${modelConfig.task} environment..."
pip install -r requirements.txt
echo "Setup complete!"`;
}