import { NextRequest, NextResponse } from 'next/server'
import JSZip from 'jszip'

export async function GET(
  request: NextRequest,
  { params }: { params: Promise<{ eventId: string }> }
) {
  try {
    const { eventId } = await params

    if (!eventId) {
      return NextResponse.json({ error: 'Event ID is required' }, { status: 400 })
    }

    // In a real implementation, you would fetch the generated files from your storage
    // For now, we'll create a mock zip with the model files
    const zip = new JSZip()

    // Mock model files (in production, fetch from your storage/database)
    const modelFiles = {
      'model.py': generateMockModelCode(),
      'train.py': generateMockTrainingCode(),
      'inference.py': generateMockInferenceCode(),
      'requirements.txt': generateMockRequirements(),
      'config.json': generateMockConfig(),
      'README.md': generateMockReadme()
    }

    // Add files to zip
    Object.entries(modelFiles).forEach(([filename, content]) => {
      zip.file(filename, content)
    })

    // Generate zip buffer
    const zipBuffer = await zip.generateAsync({ type: 'nodebuffer' })

    // Return zip file
    return new NextResponse(zipBuffer as any, {
      headers: {
        'Content-Type': 'application/zip',
        'Content-Disposition': `attachment; filename="ai-model-${eventId}.zip"`
      }
    })

  } catch (error: any) {
    console.error('Download error:', error)
    
    return NextResponse.json(
      { error: `Download failed: ${error.message}` },
      { status: 500 }
    )
  }
}

function generateMockModelCode(): string {
  return `
import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer

class TextClassifier(nn.Module):
    def __init__(self, model_name='bert-base-uncased', num_classes=3):
        super(TextClassifier, self).__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.3)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        output = self.dropout(pooled_output)
        return self.classifier(output)

# Model initialization
model = TextClassifier()
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

print("‚úÖ Model created successfully!")
`
}

function generateMockTrainingCode(): string {
  return `
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from model import TextClassifier
import json
from tqdm import tqdm

# Load configuration
with open('config.json', 'r') as f:
    config = json.load(f)

def train_model():
    model = TextClassifier()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    print("üöÄ Training started...")
    # Add your training loop here
    
    torch.save(model.state_dict(), 'model.pth')
    print("‚úÖ Training completed!")

if __name__ == "__main__":
    train_model()
`
}

function generateMockInferenceCode(): string {
  return `
import torch
from model import TextClassifier
import json

class ModelInference:
    def __init__(self):
        self.model = TextClassifier()
        self.model.load_state_dict(torch.load('model.pth'))
        self.model.eval()
    
    def predict(self, text):
        # Add your inference logic here
        return {"prediction": "positive", "confidence": 0.95}

# Example usage
if __name__ == "__main__":
    inference = ModelInference()
    result = inference.predict("This is a great product!")
    print(f"Prediction: {result}")
`
}

function generateMockRequirements(): string {
  return `torch>=2.0.0
transformers>=4.20.0
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.0
tqdm>=4.62.0
matplotlib>=3.5.0
`
}

function generateMockConfig(): string {
  return JSON.stringify({
    model_name: "AI Text Classifier",
    model_type: "text-classification",
    framework: "pytorch",
    base_model: "bert-base-uncased",
    training: {
      epochs: 10,
      batch_size: 32,
      learning_rate: 0.001
    },
    created_at: new Date().toISOString(),
    created_by: "zehanx-ai-builder"
  }, null, 2)
}

function generateMockReadme(): string {
  return `# AI Text Classifier

Generated by zehanx AI Builder

## Quick Start

1. Install dependencies:
\`\`\`bash
pip install -r requirements.txt
\`\`\`

2. Train the model:
\`\`\`bash
python train.py
\`\`\`

3. Run inference:
\`\`\`python
from inference import ModelInference

inference = ModelInference()
result = inference.predict("Your text here")
print(result)
\`\`\`

## Model Details

- **Type**: Text Classification
- **Framework**: PyTorch
- **Base Model**: BERT Base Uncased

Generated with ‚ù§Ô∏è by zehanx AI Builder
`
}